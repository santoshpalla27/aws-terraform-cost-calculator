================================================================================
PLATFORM TESTER - COMBINED FILES
Generated: 2025-12-24 22:20:58
Root Path: D:\good projects\aws-terraform-cost-calculator\tester
================================================================================

TABLE OF CONTENTS
================================================================================
1. Configuration Files (Dockerfile, requirements.txt, pytest.ini, etc.)
2. JSON Schema Contracts (ApiResponse, Job, UsageProfile, CostResult)
3. Utility Modules (api_client, polling, assertions, correlation)
4. Test Files (health, contracts, api_gateway, uploads, e2e, resilience)
5. Documentation (README.md)
================================================================================


================================================================================
FILE: conftest.py
TYPE: Pytest Root Plugin
SIZE: 2435 bytes
MODIFIED: 12/24/2025 21:33:57
================================================================================

"""
Pytest plugin to enforce certification mode.

When CERTIFICATION_MODE=true:
- Forbid test skips
- Forbid xfail
- Exit on first failure
"""
import os
import pytest


def pytest_configure(config):
    """Configure pytest for certification mode."""
    certification_mode = os.getenv('CERTIFICATION_MODE', 'false').lower() == 'true'
    
    if certification_mode:
        print("\n" + "="*60)
        print("ðŸ”’ CERTIFICATION MODE ACTIVE")
        print("="*60)
        print("RULES:")
        print("  - NO test skips allowed")
        print("  - NO xfail allowed")
        print("  - Exit on first failure")
        print("  - ALL tests MUST execute")
        print("="*60 + "\n")


def pytest_collection_modifyitems(config, items):
    """Modify test collection for certification mode."""
    certification_mode = os.getenv('CERTIFICATION_MODE', 'false').lower() == 'true'
    
    if not certification_mode:
        return
    
    # Check for skip markers
    skipped_tests = []
    for item in items:
        if item.get_closest_marker('skip'):
            skipped_tests.append(item.nodeid)
        if item.get_closest_marker('skipif'):
            skipped_tests.append(item.nodeid)
    
    if skipped_tests:
        print("\n" + "="*60)
        print("âŒ CERTIFICATION MODE VIOLATION")
        print("="*60)
        print(f"Found {len(skipped_tests)} test(s) marked for skip:")
        for test in skipped_tests:
            print(f"  - {test}")
        print("\nSkips are FORBIDDEN in certification mode.")
        print("="*60 + "\n")
        pytest.exit("Certification mode: Skipped tests detected", returncode=1)


@pytest.hookimpl(tryfirst=True, hookwrapper=True)
def pytest_runtest_makereport(item, call):
    """Hook to detect skipped tests during execution."""
    outcome = yield
    report = outcome.get_result()
    
    certification_mode = os.getenv('CERTIFICATION_MODE', 'false').lower() == 'true'
    
    if certification_mode and report.when == 'call' and report.skipped:
        print("\n" + "="*60)
        print("âŒ CERTIFICATION MODE VIOLATION")
        print("="*60)
        print(f"Test was skipped: {item.nodeid}")
        print(f"Reason: {report.longrepr}")
        print("\nSkips are FORBIDDEN in certification mode.")
        print("="*60 + "\n")
        pytest.exit("Certification mode: Test skip detected", returncode=1)





================================================================================
FILE: Dockerfile
TYPE: Docker Configuration
SIZE: 401 bytes
MODIFIED: 12/24/2025 21:09:04
================================================================================

FROM python:3.11-slim

WORKDIR /tester

# Install system dependencies
RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy test code
COPY . .

# Make entrypoint executable
RUN chmod +x entrypoint.sh

ENTRYPOINT ["./entrypoint.sh"]





================================================================================
FILE: config\endpoints.yaml
TYPE: YAML Configuration
SIZE: 761 bytes
MODIFIED: 12/24/2025 21:09:24
================================================================================

# Endpoints configuration
base_url: ${BASE_URL}
api_base: ${API_BASE}

endpoints:
  health: /health
  api_health: /api/health
  
  # Usage Profiles
  usage_profiles: /api/usage-profiles
  usage_profile_detail: /api/usage-profiles/{profile_id}
  
  # Jobs
  jobs: /api/jobs
  job_detail: /api/jobs/{job_id}
  job_status: /api/jobs/{job_id}/status
  job_results: /api/jobs/{job_id}/results
  
  # Uploads
  uploads: /api/uploads
  upload_detail: /api/uploads/{upload_id}

# Timeouts (seconds)
timeouts:
  health_check: 5
  api_request: 30
  job_completion: 300
  
# Polling configuration
polling:
  initial_delay: 1.0
  max_delay: 30.0
  max_attempts: 100

# Rate limiting
rate_limit:
  requests_per_second: 100
  burst: 150





================================================================================
FILE: entrypoint.sh
TYPE: Entrypoint Script
SIZE: 2516 bytes
MODIFIED: 12/24/2025 21:33:55
================================================================================

#!/bin/bash
set -e

echo "ðŸ§ª Platform Tester Starting..."
echo "================================"

# Get configuration from environment
BASE_URL=${BASE_URL:-http://nginx}
API_BASE=${API_BASE:-http://nginx/api}
PYTEST_ARGS=${PYTEST_ARGS:--v --tb=short}
CERTIFICATION_MODE=${CERTIFICATION_MODE:-false}

echo "Base URL: $BASE_URL"
echo "API Base: $API_BASE"
echo "Certification Mode: $CERTIFICATION_MODE"
echo "================================"

# Wait for nginx to be healthy
echo "â³ Waiting for nginx..."
MAX_RETRIES=30
RETRY_COUNT=0

until curl -sf $BASE_URL/health > /dev/null 2>&1; do
    RETRY_COUNT=$((RETRY_COUNT + 1))
    if [ $RETRY_COUNT -ge $MAX_RETRIES ]; then
        echo "âŒ Nginx failed to become healthy after $MAX_RETRIES attempts"
        exit 1
    fi
    echo "  Attempt $RETRY_COUNT/$MAX_RETRIES..."
    sleep 2
done

echo "âœ… Nginx is healthy"
echo "================================"

# Certification mode enforcement
if [ "$CERTIFICATION_MODE" = "true" ]; then
    echo "ðŸ”’ CERTIFICATION MODE ENABLED"
    echo "   - All tests MUST run"
    echo "   - No skips allowed"
    echo "   - No xfail allowed"
    echo "   - Exit on first failure"
    echo "================================"
    
    # Add strict flags
    PYTEST_ARGS="$PYTEST_ARGS --exitfirst --strict-markers -p no:warnings"
fi

# Run tests
echo "ðŸš€ Running platform tests..."
echo ""

pytest $PYTEST_ARGS

# Capture exit code
EXIT_CODE=$?

# Check for skipped tests in certification mode
if [ "$CERTIFICATION_MODE" = "true" ] && [ $EXIT_CODE -eq 0 ]; then
    # Verify no tests were skipped
    SKIP_COUNT=$(pytest --collect-only -q 2>/dev/null | grep -c "skipped" || true)
    
    if [ $SKIP_COUNT -gt 0 ]; then
        echo ""
        echo "================================"
        echo "âŒ CERTIFICATION FAILURE"
        echo "   $SKIP_COUNT test(s) were skipped"
        echo "   Skips are FORBIDDEN in certification mode"
        echo "================================"
        exit 1
    fi
fi

echo ""
echo "================================"
if [ $EXIT_CODE -eq 0 ]; then
    echo "âœ… ALL TESTS PASSED"
    if [ "$CERTIFICATION_MODE" = "true" ]; then
        echo "âœ… PLATFORM CERTIFIED"
    else
        echo "Platform is production-ready!"
    fi
else
    echo "âŒ TESTS FAILED"
    if [ "$CERTIFICATION_MODE" = "true" ]; then
        echo "âŒ CERTIFICATION DENIED"
    else
        echo "Platform has regressions - deployment blocked"
    fi
fi
echo "================================"

exit $EXIT_CODE






================================================================================
FILE: pytest.ini
TYPE: Pytest Configuration
SIZE: 663 bytes
MODIFIED: 12/24/2025 21:33:45
================================================================================

[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = 
    -v
    --tb=short
    --strict-markers
    --disable-warnings
    -p no:warnings
markers =
    health: Health check tests
    contract: Contract validation tests
    api: API Gateway tests
    uploads: Upload flow tests
    jobs: Job lifecycle tests
    terraform: Terraform execution tests
    cost: Cost pipeline tests
    results: Results governance tests
    e2e: End-to-end tests
    resilience: Resilience tests
    slow: Slow running tests

# Certification mode - forbid skips
[tool:pytest]
xfail_strict = true






================================================================================
FILE: requirements.txt
TYPE: Python Dependencies
SIZE: 165 bytes
MODIFIED: 12/24/2025 21:09:01
================================================================================

pytest==7.4.3
pytest-asyncio==0.21.1
requests==2.31.0
httpx==0.25.2
jsonschema==4.20.0
pyyaml==6.0.1
locust==2.20.0
playwright==1.40.0
python-dotenv==1.0.0





================================================================================
FILE: tests\conftest.py
TYPE: Pytest Fixtures
SIZE: 1082 bytes
MODIFIED: 12/24/2025 21:11:11
================================================================================

"""
Pytest configuration and shared fixtures.
"""
import pytest
from utils.api_client import PlatformClient
from utils.correlation import get_tracker, print_correlation_summary


@pytest.fixture(scope="session")
def api_client():
    """Create API client for testing."""
    return PlatformClient()


@pytest.fixture(scope="session")
def base_url():
    """Get base URL from environment."""
    import os
    return os.getenv('BASE_URL', 'http://nginx')


@pytest.fixture(autouse=True, scope="session")
def print_summary_at_end():
    """Print correlation ID summary at end of test session."""
    yield
    print_correlation_summary()


@pytest.fixture
def track_correlation(api_client):
    """Fixture to track correlation IDs."""
    tracker = get_tracker()
    
    def _track(response, endpoint, method):
        correlation_id = api_client.get_correlation_id(response)
        success = response.get('success', False)
        tracker.track(correlation_id, endpoint, method, success)
        return correlation_id
    
    return _track





================================================================================
FILE: config\contracts\ApiResponse.json
TYPE: JSON Schema Contract
SIZE: 1499 bytes
MODIFIED: 12/24/2025 21:09:30
================================================================================

{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "$id": "https://platform.example.com/schemas/ApiResponse.json",
  "title": "API Response Envelope",
  "description": "Standard response envelope for all API endpoints",
  "type": "object",
  "required": ["success", "data", "error", "correlation_id"],
  "properties": {
    "success": {
      "type": "boolean",
      "description": "Indicates if the request was successful"
    },
    "data": {
      "description": "Response payload (null on error)",
      "oneOf": [
        {"type": "object"},
        {"type": "array"},
        {"type": "null"}
      ]
    },
    "error": {
      "description": "Error details (null on success)",
      "oneOf": [
        {
          "type": "object",
          "required": ["message", "code"],
          "properties": {
            "message": {
              "type": "string",
              "description": "Human-readable error message"
            },
            "code": {
              "type": "string",
              "description": "Machine-readable error code"
            },
            "details": {
              "type": "object",
              "description": "Additional error context"
            }
          }
        },
        {"type": "null"}
      ]
    },
    "correlation_id": {
      "type": "string",
      "format": "uuid",
      "description": "Unique request identifier for tracing"
    }
  },
  "additionalProperties": false
}





================================================================================
FILE: config\contracts\CostResult.json
TYPE: JSON Schema Contract
SIZE: 1634 bytes
MODIFIED: 12/24/2025 21:09:40
================================================================================

{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "$id": "https://platform.example.com/schemas/CostResult.json",
    "title": "Cost Result",
    "description": "Immutable cost estimation result",
    "type": "object",
    "required": [
        "job_id",
        "total_monthly_cost",
        "currency",
        "breakdown"
    ],
    "properties": {
        "job_id": {
            "type": "string",
            "format": "uuid"
        },
        "total_monthly_cost": {
            "type": "number",
            "minimum": 0
        },
        "currency": {
            "type": "string",
            "enum": [
                "USD",
                "EUR",
                "GBP"
            ]
        },
        "breakdown": {
            "type": "array",
            "items": {
                "type": "object",
                "required": [
                    "resource_name",
                    "service",
                    "resource_type",
                    "monthly_cost"
                ],
                "properties": {
                    "resource_name": {
                        "type": "string"
                    },
                    "service": {
                        "type": "string"
                    },
                    "resource_type": {
                        "type": "string"
                    },
                    "monthly_cost": {
                        "type": "number",
                        "minimum": 0
                    }
                }
            }
        }
    },
    "additionalProperties": false
}




================================================================================
FILE: config\contracts\Job.json
TYPE: JSON Schema Contract
SIZE: 1982 bytes
MODIFIED: 12/24/2025 21:09:33
================================================================================

{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "$id": "https://platform.example.com/schemas/Job.json",
    "title": "Job",
    "description": "Cost estimation job",
    "type": "object",
    "required": [
        "job_id",
        "name",
        "upload_id",
        "status",
        "progress",
        "created_at",
        "updated_at"
    ],
    "properties": {
        "job_id": {
            "type": "string",
            "format": "uuid"
        },
        "name": {
            "type": "string",
            "minLength": 1
        },
        "upload_id": {
            "type": "string",
            "format": "uuid"
        },
        "usage_profile": {
            "type": "string"
        },
        "status": {
            "type": "string",
            "enum": [
                "UPLOADED",
                "PLANNING",
                "PARSING",
                "ENRICHING",
                "COSTING",
                "COMPLETED",
                "FAILED"
            ]
        },
        "progress": {
            "type": "number",
            "minimum": 0,
            "maximum": 100
        },
        "current_stage": {
            "type": [
                "string",
                "null"
            ]
        },
        "created_at": {
            "type": "string",
            "format": "date-time"
        },
        "updated_at": {
            "type": "string",
            "format": "date-time"
        },
        "completed_at": {
            "type": [
                "string",
                "null"
            ],
            "format": "date-time"
        },
        "errors": {
            "type": "array",
            "items": {
                "type": "string"
            }
        },
        "error_message": {
            "type": [
                "string",
                "null"
            ]
        }
    },
    "additionalProperties": false
}




================================================================================
FILE: config\contracts\UsageProfile.json
TYPE: JSON Schema Contract
SIZE: 1279 bytes
MODIFIED: 12/24/2025 21:09:38
================================================================================

{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "$id": "https://platform.example.com/schemas/UsageProfile.json",
    "title": "Usage Profile",
    "description": "Infrastructure usage assumptions",
    "type": "object",
    "required": [
        "id",
        "name",
        "description",
        "is_default"
    ],
    "properties": {
        "id": {
            "type": "string"
        },
        "name": {
            "type": "string",
            "minLength": 1
        },
        "description": {
            "type": "string"
        },
        "is_default": {
            "type": "boolean"
        },
        "assumptions": {
            "type": "object",
            "properties": {
                "hours_per_day": {
                    "type": "number",
                    "minimum": 0,
                    "maximum": 24
                },
                "days_per_month": {
                    "type": "number",
                    "minimum": 0,
                    "maximum": 31
                },
                "availability_zone_count": {
                    "type": "integer",
                    "minimum": 1
                }
            }
        }
    },
    "additionalProperties": false
}




================================================================================
FILE: utils\__init__.py
TYPE: Utility Module
SIZE: 45 bytes
MODIFIED: 12/24/2025 21:10:12
================================================================================

"""Utility modules for platform testing."""





================================================================================
FILE: utils\api_client.py
TYPE: Utility Module
SIZE: 5122 bytes
MODIFIED: 12/24/2025 21:10:21
================================================================================

"""
API Client for platform testing.

Provides a wrapper around requests with automatic schema validation,
correlation_id tracking, and error handling.
"""
import os
import json
from typing import Any, Dict, Optional
from pathlib import Path

import requests
from jsonschema import validate, ValidationError


class PlatformClient:
    """HTTP client for platform API with contract validation."""
    
    def __init__(self, base_url: str = None):
        """
        Initialize client.
        
        Args:
            base_url: Base URL for API (defaults to env var API_BASE)
        """
        self.base_url = base_url or os.getenv('API_BASE', 'http://nginx/api')
        self.session = requests.Session()
        self.session.headers.update({
            'Content-Type': 'application/json',
            'Accept': 'application/json'
        })
        
        # Load schemas
        self.schemas = self._load_schemas()
    
    def _load_schemas(self) -> Dict[str, dict]:
        """Load JSON schemas from contracts directory."""
        schemas = {}
        contracts_dir = Path(__file__).parent.parent / 'config' / 'contracts'
        
        for schema_file in contracts_dir.glob('*.json'):
            with open(schema_file) as f:
                schemas[schema_file.stem] = json.load(f)
        
        return schemas
    
    def _validate_response(self, data: dict):
        """
        Validate response against ApiResponse schema.
        
        Args:
            data: Response JSON
            
        Raises:
            ValidationError: If response doesn't match schema
            AssertionError: If correlation_id missing
        """
        try:
            validate(instance=data, schema=self.schemas['ApiResponse'])
        except ValidationError as e:
            raise AssertionError(f"Response schema validation failed: {e.message}")
        
        # Additional validation
        assert 'correlation_id' in data, "Missing correlation_id"
        assert isinstance(data['success'], bool), "success must be boolean"
        
        if data['success']:
            assert data['data'] is not None, "data must not be null on success"
            assert data['error'] is None, "error must be null on success"
        else:
            assert data['error'] is not None, "error must not be null on failure"
            assert 'message' in data['error'], "error.message required"
            assert 'code' in data['error'], "error.code required"
    
    def request(
        self,
        method: str,
        endpoint: str,
        validate_schema: str = None,
        **kwargs
    ) -> dict:
        """
        Make HTTP request with automatic validation.
        
        Args:
            method: HTTP method
            endpoint: API endpoint (relative to base_url)
            validate_schema: Optional schema name to validate data against
            **kwargs: Additional arguments for requests
            
        Returns:
            Response JSON
            
        Raises:
            AssertionError: If validation fails
        """
        url = f"{self.base_url}{endpoint}"
        
        try:
            response = self.session.request(method, url, **kwargs)
            response.raise_for_status()
        except requests.RequestException as e:
            raise AssertionError(f"Request failed: {e}")
        
        try:
            data = response.json()
        except json.JSONDecodeError:
            raise AssertionError(f"Invalid JSON response: {response.text}")
        
        # Validate ApiResponse envelope
        self._validate_response(data)
        
        # Validate data against specific schema if requested
        if validate_schema and data['success']:
            if validate_schema in self.schemas:
                try:
                    validate(instance=data['data'], schema=self.schemas[validate_schema])
                except ValidationError as e:
                    raise AssertionError(f"{validate_schema} validation failed: {e.message}")
        
        return data
    
    def get(self, endpoint: str, validate_schema: str = None, **kwargs) -> dict:
        """GET request."""
        return self.request('GET', endpoint, validate_schema=validate_schema, **kwargs)
    
    def post(self, endpoint: str, validate_schema: str = None, **kwargs) -> dict:
        """POST request."""
        return self.request('POST', endpoint, validate_schema=validate_schema, **kwargs)
    
    def put(self, endpoint: str, validate_schema: str = None, **kwargs) -> dict:
        """PUT request."""
        return self.request('PUT', endpoint, validate_schema=validate_schema, **kwargs)
    
    def delete(self, endpoint: str, **kwargs) -> dict:
        """DELETE request."""
        return self.request('DELETE', endpoint, **kwargs)
    
    def get_correlation_id(self, response: dict) -> str:
        """Extract correlation_id from response."""
        return response.get('correlation_id', 'MISSING')





================================================================================
FILE: utils\assertions.py
TYPE: Utility Module
SIZE: 4269 bytes
MODIFIED: 12/24/2025 21:10:45
================================================================================

"""
Custom assertions for platform testing.
"""
import uuid
from typing import Dict, List


# Valid job state transitions
VALID_TRANSITIONS = {
    'UPLOADED': ['PLANNING', 'FAILED'],
    'PLANNING': ['PARSING', 'FAILED'],
    'PARSING': ['ENRICHING', 'FAILED'],
    'ENRICHING': ['COSTING', 'FAILED'],
    'COSTING': ['COMPLETED', 'FAILED'],
    'COMPLETED': [],  # Terminal
    'FAILED': []      # Terminal
}


def assert_valid_state_transition(from_state: str, to_state: str):
    """
    Assert job state transition is valid.
    
    Args:
        from_state: Current state
        to_state: Next state
        
    Raises:
        AssertionError: If transition is invalid
    """
    valid_next_states = VALID_TRANSITIONS.get(from_state, [])
    assert to_state in valid_next_states, \
        f"Invalid state transition: {from_state} â†’ {to_state}. Valid: {valid_next_states}"


def assert_correlation_id(response: dict):
    """
    Assert correlation_id exists and is valid UUID.
    
    Args:
        response: API response
        
    Raises:
        AssertionError: If correlation_id missing or invalid
    """
    assert 'correlation_id' in response, "Missing correlation_id in response"
    
    correlation_id = response['correlation_id']
    try:
        uuid.UUID(correlation_id)
    except (ValueError, AttributeError):
        raise AssertionError(f"Invalid correlation_id format: {correlation_id}")


def assert_api_success(response: dict):
    """
    Assert API response indicates success.
    
    Args:
        response: API response
        
    Raises:
        AssertionError: If response indicates failure
    """
    assert response.get('success') is True, \
        f"API call failed: {response.get('error', {}).get('message', 'Unknown error')}"


def assert_api_failure(response: dict, expected_code: str = None):
    """
    Assert API response indicates failure.
    
    Args:
        response: API response
        expected_code: Optional expected error code
        
    Raises:
        AssertionError: If response indicates success or wrong error code
    """
    assert response.get('success') is False, "Expected API failure but got success"
    assert response.get('error') is not None, "Missing error object in failure response"
    
    if expected_code:
        actual_code = response['error'].get('code')
        assert actual_code == expected_code, \
            f"Expected error code '{expected_code}' but got '{actual_code}'"


def assert_no_forbidden_fields(data: dict, forbidden: List[str]):
    """
    Assert response doesn't contain forbidden fields.
    
    Args:
        data: Response data
        forbidden: List of forbidden field names
        
    Raises:
        AssertionError: If forbidden fields present
    """
    found_forbidden = [field for field in forbidden if field in data]
    assert not found_forbidden, \
        f"Response contains forbidden fields: {found_forbidden}"


def assert_monotonic_progress(old_progress: float, new_progress: float):
    """
    Assert progress is monotonically increasing.
    
    Args:
        old_progress: Previous progress value
        new_progress: New progress value
        
    Raises:
        AssertionError: If progress decreased
    """
    assert new_progress >= old_progress, \
        f"Progress decreased: {old_progress} â†’ {new_progress}"


def assert_terminal_state(state: str):
    """
    Assert state is terminal (COMPLETED or FAILED).
    
    Args:
        state: Job state
        
    Raises:
        AssertionError: If state is not terminal
    """
    assert state in ['COMPLETED', 'FAILED'], \
        f"Expected terminal state but got: {state}"


def assert_immutable_result(result_id: str, old_data: dict, new_data: dict):
    """
    Assert result data hasn't changed (immutability).
    
    Args:
        result_id: Result identifier
        old_data: Original result data
        new_data: New result data
        
    Raises:
        AssertionError: If data has changed
    """
    assert old_data == new_data, \
        f"Result {result_id} was mutated! Immutability violated."





================================================================================
FILE: utils\correlation.py
TYPE: Utility Module
SIZE: 2741 bytes
MODIFIED: 12/24/2025 21:10:47
================================================================================

"""
Correlation ID tracking and management.
"""
from typing import Dict, List
from collections import defaultdict


class CorrelationTracker:
    """Track correlation IDs across requests for debugging."""
    
    def __init__(self):
        """Initialize tracker."""
        self.correlation_ids: List[str] = []
        self.request_map: Dict[str, dict] = {}
        self.error_map: Dict[str, List[str]] = defaultdict(list)
    
    def track(self, correlation_id: str, endpoint: str, method: str, success: bool):
        """
        Track a request.
        
        Args:
            correlation_id: Request correlation ID
            endpoint: API endpoint
            method: HTTP method
            success: Whether request succeeded
        """
        self.correlation_ids.append(correlation_id)
        self.request_map[correlation_id] = {
            'endpoint': endpoint,
            'method': method,
            'success': success
        }
        
        if not success:
            self.error_map[endpoint].append(correlation_id)
    
    def get_failed_requests(self) -> List[Dict]:
        """Get all failed requests."""
        return [
            {
                'correlation_id': cid,
                **details
            }
            for cid, details in self.request_map.items()
            if not details['success']
        ]
    
    def get_error_summary(self) -> Dict[str, int]:
        """Get error count by endpoint."""
        return {
            endpoint: len(cids)
            for endpoint, cids in self.error_map.items()
        }
    
    def print_summary(self):
        """Print tracking summary."""
        print("\n" + "="*60)
        print("CORRELATION ID TRACKING SUMMARY")
        print("="*60)
        print(f"Total requests: {len(self.correlation_ids)}")
        print(f"Failed requests: {len(self.get_failed_requests())}")
        
        if self.error_map:
            print("\nErrors by endpoint:")
            for endpoint, cids in self.error_map.items():
                print(f"  {endpoint}: {len(cids)} errors")
                for cid in cids[:3]:  # Show first 3
                    print(f"    - {cid}")
        
        print("="*60 + "\n")


# Global tracker instance
_tracker = CorrelationTracker()


def track_request(correlation_id: str, endpoint: str, method: str, success: bool):
    """Track a request globally."""
    _tracker.track(correlation_id, endpoint, method, success)


def get_tracker() -> CorrelationTracker:
    """Get global tracker instance."""
    return _tracker


def print_correlation_summary():
    """Print global tracking summary."""
    _tracker.print_summary()





================================================================================
FILE: utils\docker_control.py
TYPE: Utility Module
SIZE: 5045 bytes
MODIFIED: 12/24/2025 21:28:43
================================================================================

"""
Docker interaction utilities for resilience testing.

Provides safe container restart capabilities for testing platform resilience.
"""
import subprocess
import time
from typing import Optional


class DockerController:
    """Control Docker containers for resilience testing."""
    
    def __init__(self):
        """Initialize Docker controller."""
        self._verify_docker_available()
    
    def _verify_docker_available(self):
        """Verify Docker is available."""
        try:
            result = subprocess.run(
                ['docker', 'version'],
                capture_output=True,
                text=True,
                timeout=5
            )
            if result.returncode != 0:
                raise RuntimeError("Docker is not available")
        except (subprocess.TimeoutExpired, FileNotFoundError) as e:
            raise RuntimeError(f"Docker is not available: {e}")
    
    def restart_container(self, container_name: str, wait_healthy: bool = True, timeout: int = 30) -> bool:
        """
        Restart a Docker container.
        
        Args:
            container_name: Name of container to restart
            wait_healthy: Wait for container to become healthy
            timeout: Maximum time to wait for health check
            
        Returns:
            True if restart successful
            
        Raises:
            RuntimeError: If restart fails
        """
        print(f"   ðŸ”„ Restarting container: {container_name}")
        
        # Restart container
        result = subprocess.run(
            ['docker', 'restart', container_name],
            capture_output=True,
            text=True,
            timeout=30
        )
        
        if result.returncode != 0:
            raise RuntimeError(f"Failed to restart {container_name}: {result.stderr}")
        
        print(f"   âœ“ Container restarted: {container_name}")
        
        # Wait for healthy if requested
        if wait_healthy:
            return self.wait_for_healthy(container_name, timeout)
        
        return True
    
    def wait_for_healthy(self, container_name: str, timeout: int = 30) -> bool:
        """
        Wait for container to become healthy.
        
        Args:
            container_name: Name of container
            timeout: Maximum time to wait
            
        Returns:
            True if container becomes healthy
            
        Raises:
            TimeoutError: If container doesn't become healthy
        """
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            # Check container health
            result = subprocess.run(
                ['docker', 'inspect', '--format={{.State.Health.Status}}', container_name],
                capture_output=True,
                text=True,
                timeout=5
            )
            
            if result.returncode == 0:
                health_status = result.stdout.strip()
                
                if health_status == 'healthy':
                    print(f"   âœ“ Container healthy: {container_name}")
                    return True
                
                # If no health check defined, check if running
                if health_status == '<no value>':
                    running_result = subprocess.run(
                        ['docker', 'inspect', '--format={{.State.Running}}', container_name],
                        capture_output=True,
                        text=True,
                        timeout=5
                    )
                    
                    if running_result.returncode == 0 and running_result.stdout.strip() == 'true':
                        print(f"   âœ“ Container running: {container_name}")
                        return True
            
            time.sleep(1)
        
        raise TimeoutError(f"Container {container_name} did not become healthy within {timeout}s")
    
    def get_container_status(self, container_name: str) -> dict:
        """
        Get container status.
        
        Args:
            container_name: Name of container
            
        Returns:
            Dictionary with status information
        """
        result = subprocess.run(
            ['docker', 'inspect', container_name],
            capture_output=True,
            text=True,
            timeout=5
        )
        
        if result.returncode != 0:
            return {'running': False, 'healthy': False}
        
        import json
        inspect_data = json.loads(result.stdout)[0]
        
        state = inspect_data.get('State', {})
        health = state.get('Health', {})
        
        return {
            'running': state.get('Running', False),
            'healthy': health.get('Status') == 'healthy' if health else state.get('Running', False),
            'status': health.get('Status', 'unknown')
        }





================================================================================
FILE: utils\polling.py
TYPE: Utility Module
SIZE: 3175 bytes
MODIFIED: 12/24/2025 21:10:36
================================================================================

"""
Polling utilities with exponential backoff.
"""
import asyncio
import time
from typing import Callable, Any, Optional


async def poll_until(
    check_fn: Callable[[], Any],
    condition_fn: Callable[[Any], bool] = None,
    max_attempts: int = 100,
    initial_delay: float = 1.0,
    max_delay: float = 30.0,
    timeout: float = 300.0
) -> Any:
    """
    Poll until condition is met with exponential backoff.
    
    Args:
        check_fn: Function to call for checking
        condition_fn: Function to test result (default: truthy check)
        max_attempts: Maximum number of attempts
        initial_delay: Initial delay in seconds
        max_delay: Maximum delay in seconds
        timeout: Total timeout in seconds
        
    Returns:
        Result from check_fn when condition is met
        
    Raises:
        TimeoutError: If condition not met within attempts/timeout
    """
    if condition_fn is None:
        condition_fn = lambda x: bool(x)
    
    start_time = time.time()
    
    for attempt in range(max_attempts):
        # Check timeout
        if time.time() - start_time > timeout:
            raise TimeoutError(f"Polling timed out after {timeout}s")
        
        # Execute check
        result = check_fn()
        
        # Test condition
        if condition_fn(result):
            return result
        
        # Calculate delay with exponential backoff
        delay = min(initial_delay * (2 ** attempt), max_delay)
        
        # Sleep
        await asyncio.sleep(delay)
    
    raise TimeoutError(f"Polling failed after {max_attempts} attempts")


def poll_until_sync(
    check_fn: Callable[[], Any],
    condition_fn: Callable[[Any], bool] = None,
    max_attempts: int = 100,
    initial_delay: float = 1.0,
    max_delay: float = 30.0,
    timeout: float = 300.0
) -> Any:
    """
    Synchronous version of poll_until.
    
    Args:
        check_fn: Function to call for checking
        condition_fn: Function to test result (default: truthy check)
        max_attempts: Maximum number of attempts
        initial_delay: Initial delay in seconds
        max_delay: Maximum delay in seconds
        timeout: Total timeout in seconds
        
    Returns:
        Result from check_fn when condition is met
        
    Raises:
        TimeoutError: If condition not met within attempts/timeout
    """
    if condition_fn is None:
        condition_fn = lambda x: bool(x)
    
    start_time = time.time()
    
    for attempt in range(max_attempts):
        # Check timeout
        if time.time() - start_time > timeout:
            raise TimeoutError(f"Polling timed out after {timeout}s")
        
        # Execute check
        result = check_fn()
        
        # Test condition
        if condition_fn(result):
            return result
        
        # Calculate delay with exponential backoff
        delay = min(initial_delay * (2 ** attempt), max_delay)
        
        # Sleep
        time.sleep(delay)
    
    raise TimeoutError(f"Polling failed after {max_attempts} attempts")





================================================================================
FILE: tests\00_health\test_all_services_up.py
TYPE: Test Suite
SIZE: 1680 bytes
MODIFIED: 12/24/2025 22:14:51
================================================================================

"""
Health check tests - ensure all services are reachable.

These tests run first and fail fast if infrastructure is down.
"""
import pytest
import requests


@pytest.mark.health
def test_nginx_responds(base_url):
    """Test that nginx is responding."""
    response = requests.get(f"{base_url}/health", timeout=5)
    assert response.status_code == 200, "Nginx health check failed"


@pytest.mark.health
def test_api_gateway_health(api_client):
    """Test that API Gateway is healthy."""
    # API Gateway doesn't have /health, but we can test if it responds
    # by checking a known endpoint like /usage-profiles
    import requests
    
    # Test that API Gateway is reachable
    response = requests.get(f"{api_client.base_url}/usage-profiles", timeout=5)
    
    # Should return 200 or proper API response (not 404/500)
    assert response.status_code != 404, "API Gateway not found - service may be down"
    assert response.status_code != 500, "API Gateway internal error"
    
    # If we get here, API Gateway is responding
    print("   âœ“ API Gateway is responding")


@pytest.mark.health
def test_all_backend_services_reachable(api_client):
    """
    Test that all backend services are reachable through API Gateway.
    
    This is a smoke test to ensure the platform is minimally functional.
    """
    # Try to list usage profiles (touches multiple services)
    response = api_client.get('/usage-profiles')
    
    # Should succeed or fail gracefully, but not crash
    assert 'success' in response, "API Gateway not responding correctly"
    assert 'correlation_id' in response, "Missing correlation_id"





================================================================================
FILE: tests\07_e2e\test_full_user_flow.py
TYPE: Test Suite
SIZE: 9913 bytes
MODIFIED: 12/24/2025 21:23:23
================================================================================

"""
End-to-end test - REAL execution with HARD FAILURES.

This test executes the COMPLETE user flow with NO soft assertions:
1. Upload real Terraform files
2. Create real job
3. Poll until terminal state
4. Fetch real results
5. Verify immutability

ANY failure blocks deployment.
"""
import pytest
import os
from pathlib import Path
from utils.polling import poll_until_sync
from utils.assertions import (
    assert_valid_state_transition,
    assert_terminal_state,
    assert_monotonic_progress,
    assert_immutable_result
)


@pytest.mark.e2e
@pytest.mark.slow
def test_full_user_flow_real_execution(api_client, track_correlation):
    """
    REAL end-to-end test - NO SKIPS, NO MOCKS.
    
    This test MUST pass for platform to be production-ready.
    Any failure indicates platform is NOT functional.
    """
    print("\n" + "="*80)
    print("E2E TEST: FULL USER FLOW - REAL EXECUTION")
    print("="*80)
    
    # ========================================================================
    # STEP 1: Get usage profiles
    # ========================================================================
    print("\n[1/6] Fetching usage profiles...")
    profiles_response = api_client.get('/usage-profiles', validate_schema='UsageProfile')
    track_correlation(profiles_response, '/usage-profiles', 'GET')
    
    assert profiles_response['success'], \
        f"FAILED: Could not fetch usage profiles. correlation_id={profiles_response.get('correlation_id')}"
    
    profiles = profiles_response['data']
    assert isinstance(profiles, list), "Usage profiles must be a list"
    assert len(profiles) > 0, "FAILED: No usage profiles available"
    
    # Use first profile
    profile = profiles[0]
    profile_id = profile.get('id')
    assert profile_id is not None, "FAILED: Usage profile missing 'id' field"
    
    print(f"   âœ“ Using profile: {profile.get('name', profile_id)}")
    
    # ========================================================================
    # STEP 2: Upload Terraform files
    # ========================================================================
    print("\n[2/6] Uploading Terraform files...")
    
    # Get fixture path
    fixture_dir = Path(__file__).parent.parent.parent / 'fixtures' / 'simple_ec2'
    assert fixture_dir.exists(), f"FAILED: Fixture directory not found: {fixture_dir}"
    
    main_tf = fixture_dir / 'main.tf'
    providers_tf = fixture_dir / 'providers.tf'
    
    assert main_tf.exists(), f"FAILED: main.tf not found: {main_tf}"
    assert providers_tf.exists(), f"FAILED: providers.tf not found: {providers_tf}"
    
    # Create multipart upload
    import requests
    
    files = {
        'file': ('terraform.zip', create_terraform_zip(fixture_dir), 'application/zip')
    }
    
    upload_url = f"{api_client.base_url}/uploads"
    upload_response = requests.post(upload_url, files=files)
    
    assert upload_response.status_code in [200, 201], \
        f"FAILED: Upload failed with status {upload_response.status_code}. Response: {upload_response.text}"
    
    upload_data = upload_response.json()
    assert upload_data.get('success'), \
        f"FAILED: Upload API returned success=false. Error: {upload_data.get('error')}"
    
    upload_id = upload_data['data'].get('upload_id')
    assert upload_id is not None, "FAILED: Upload response missing 'upload_id'"
    
    print(f"   âœ“ Upload successful: {upload_id}")
    track_correlation(upload_data, '/uploads', 'POST')
    
    # ========================================================================
    # STEP 3: Create job
    # ========================================================================
    print("\n[3/6] Creating job...")
    
    job_payload = {
        'name': 'E2E Test Job - Real Execution',
        'upload_id': upload_id,
        'usage_profile': profile_id
    }
    
    job_response = api_client.post('/jobs', json=job_payload, validate_schema='Job')
    track_correlation(job_response, '/jobs', 'POST')
    
    assert job_response['success'], \
        f"FAILED: Job creation failed. Error: {job_response.get('error')}"
    
    job_data = job_response['data']
    job_id = job_data.get('job_id')
    assert job_id is not None, "FAILED: Job response missing 'job_id'"
    
    print(f"   âœ“ Job created: {job_id}")
    print(f"   Initial status: {job_data.get('status')}")
    
    # ========================================================================
    # STEP 4: Poll job status until terminal
    # ========================================================================
    print("\n[4/6] Polling job status...")
    
    previous_state = None
    previous_progress = 0
    poll_count = 0
    max_polls = 60
    
    def check_job_status():
        nonlocal previous_state, previous_progress, poll_count
        poll_count += 1
        
        status_response = api_client.get(f'/jobs/{job_id}/status')
        assert status_response['success'], \
            f"FAILED: Status check failed at poll {poll_count}"
        
        status_data = status_response['data']
        current_state = status_data.get('status')
        current_progress = status_data.get('progress', 0)
        
        # Validate state transition
        if previous_state is not None and previous_state != current_state:
            assert_valid_state_transition(previous_state, current_state)
        
        # Validate monotonic progress
        if current_state not in ['FAILED']:
            assert_monotonic_progress(previous_progress, current_progress)
        
        previous_state = current_state
        previous_progress = current_progress
        
        print(f"   Poll {poll_count}: {current_state} ({current_progress}%)")
        
        return status_data
    
    def is_terminal(status_data):
        state = status_data.get('status')
        return state in ['COMPLETED', 'FAILED']
    
    try:
        final_status = poll_until_sync(
            check_fn=check_job_status,
            condition_fn=is_terminal,
            max_attempts=max_polls,
            initial_delay=2.0,
            max_delay=10.0,
            timeout=300
        )
    except TimeoutError as e:
        pytest.fail(f"FAILED: Job did not reach terminal state within timeout. Last state: {previous_state}")
    
    final_state = final_status.get('status')
    assert_terminal_state(final_state)
    
    # MUST be COMPLETED, not FAILED
    assert final_state == 'COMPLETED', \
        f"FAILED: Job ended in FAILED state. Error: {final_status.get('error_message')}"
    
    print(f"   âœ“ Job completed successfully after {poll_count} polls")
    
    # ========================================================================
    # STEP 5: Fetch results
    # ========================================================================
    print("\n[5/6] Fetching cost results...")
    
    results_response = api_client.get(f'/jobs/{job_id}/results', validate_schema='CostResult')
    track_correlation(results_response, f'/jobs/{job_id}/results', 'GET')
    
    assert results_response['success'], \
        f"FAILED: Could not fetch results. Error: {results_response.get('error')}"
    
    results = results_response['data']
    
    # Validate results structure
    assert 'total_monthly_cost' in results, "FAILED: Results missing 'total_monthly_cost'"
    assert 'currency' in results, "FAILED: Results missing 'currency'"
    assert 'breakdown' in results, "FAILED: Results missing 'breakdown'"
    
    total_cost = results['total_monthly_cost']
    currency = results['currency']
    breakdown = results['breakdown']
    
    assert isinstance(total_cost, (int, float)), "FAILED: total_monthly_cost must be numeric"
    assert total_cost >= 0, "FAILED: total_monthly_cost cannot be negative"
    assert isinstance(breakdown, list), "FAILED: breakdown must be a list"
    assert len(breakdown) > 0, "FAILED: breakdown cannot be empty"
    
    print(f"   âœ“ Results fetched successfully")
    print(f"   Total cost: {currency} {total_cost:.2f}/month")
    print(f"   Resources: {len(breakdown)}")
    
    # ========================================================================
    # STEP 6: Verify immutability
    # ========================================================================
    print("\n[6/6] Verifying result immutability...")
    
    # Fetch results again
    results_response_2 = api_client.get(f'/jobs/{job_id}/results')
    results_2 = results_response_2['data']
    
    # Results MUST be identical
    assert_immutable_result(job_id, results, results_2)
    
    print(f"   âœ“ Results are immutable")
    
    # ========================================================================
    # SUCCESS
    # ========================================================================
    print("\n" + "="*80)
    print("âœ… E2E TEST PASSED - PLATFORM IS PRODUCTION-READY")
    print("="*80)
    print(f"Job ID: {job_id}")
    print(f"Total Cost: {currency} {total_cost:.2f}/month")
    print(f"Resources Analyzed: {len(breakdown)}")
    print(f"Polls Required: {poll_count}")
    print("="*80 + "\n")


def create_terraform_zip(fixture_dir: Path) -> bytes:
    """
    Create a ZIP file from Terraform fixtures.
    
    Args:
        fixture_dir: Path to fixture directory
        
    Returns:
        ZIP file bytes
    """
    import io
    import zipfile
    
    zip_buffer = io.BytesIO()
    
    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
        for tf_file in fixture_dir.glob('*.tf'):
            zip_file.write(tf_file, arcname=tf_file.name)
    
    zip_buffer.seek(0)
    return zip_buffer.read()





================================================================================
FILE: tests\08_resilience\test_restart_recovery.py
TYPE: Test Suite
SIZE: 11604 bytes
MODIFIED: 12/24/2025 21:28:49
================================================================================

"""
Resilience tests - REAL service restart scenarios with HARD FAILURES.

These tests validate that the platform can survive service failures:
- Job orchestrator restart mid-job
- Pricing engine restart mid-job
- Jobs must reach terminal state (COMPLETED or FAILED)
- No stuck states allowed
- Failure reasons must be visible

ANY failure to recover blocks deployment.
"""
import pytest
import time
from pathlib import Path
from utils.polling import poll_until_sync
from utils.docker_control import DockerController
from utils.assertions import assert_terminal_state


@pytest.mark.resilience
@pytest.mark.slow
def test_job_survives_orchestrator_restart(api_client, track_correlation):
    """
    Test that jobs survive job-orchestrator restart.
    
    HARD REQUIREMENTS:
    - Job MUST reach terminal state after restart
    - Job MUST NOT be stuck
    - Failure reason MUST be visible if job fails
    """
    print("\n" + "="*80)
    print("RESILIENCE TEST: Job Orchestrator Restart Mid-Job")
    print("="*80)
    
    docker = DockerController()
    
    # ========================================================================
    # STEP 1: Create and start a job
    # ========================================================================
    print("\n[1/5] Creating job...")
    
    # Get usage profile
    profiles_response = api_client.get('/usage-profiles')
    assert profiles_response['success'], "FAILED: Could not fetch usage profiles"
    
    profile_id = profiles_response['data'][0]['id']
    
    # Upload Terraform
    fixture_dir = Path(__file__).parent.parent.parent / 'fixtures' / 'simple_ec2'
    assert fixture_dir.exists(), f"FAILED: Fixture not found: {fixture_dir}"
    
    from tests.test_07_e2e.test_full_user_flow import create_terraform_zip
    import requests
    
    files = {'file': ('terraform.zip', create_terraform_zip(fixture_dir), 'application/zip')}
    upload_response = requests.post(f"{api_client.base_url}/uploads", files=files)
    assert upload_response.status_code in [200, 201], "FAILED: Upload failed"
    
    upload_id = upload_response.json()['data']['upload_id']
    
    # Create job
    job_response = api_client.post('/jobs', json={
        'name': 'Resilience Test - Orchestrator Restart',
        'upload_id': upload_id,
        'usage_profile': profile_id
    })
    assert job_response['success'], "FAILED: Job creation failed"
    
    job_id = job_response['data']['job_id']
    print(f"   âœ“ Job created: {job_id}")
    
    # ========================================================================
    # STEP 2: Wait for job to start processing
    # ========================================================================
    print("\n[2/5] Waiting for job to start processing...")
    
    max_wait = 10
    for i in range(max_wait):
        status_response = api_client.get(f'/jobs/{job_id}/status')
        current_status = status_response['data']['status']
        
        if current_status not in ['UPLOADED']:
            print(f"   âœ“ Job processing: {current_status}")
            break
        
        time.sleep(1)
    else:
        pytest.fail("FAILED: Job did not start processing")
    
    # ========================================================================
    # STEP 3: Restart job-orchestrator
    # ========================================================================
    print("\n[3/5] Restarting job-orchestrator...")
    
    try:
        docker.restart_container('cost-platform-job-orchestrator', wait_healthy=True, timeout=30)
    except Exception as e:
        pytest.fail(f"FAILED: Could not restart job-orchestrator: {e}")
    
    print("   âœ“ Job orchestrator restarted and healthy")
    
    # ========================================================================
    # STEP 4: Poll until terminal state
    # ========================================================================
    print("\n[4/5] Polling job to terminal state...")
    
    poll_count = 0
    
    def check_status():
        nonlocal poll_count
        poll_count += 1
        
        status_response = api_client.get(f'/jobs/{job_id}/status')
        assert status_response['success'], f"FAILED: Status check failed at poll {poll_count}"
        
        status_data = status_response['data']
        current_state = status_data['status']
        
        print(f"   Poll {poll_count}: {current_state} ({status_data.get('progress', 0)}%)")
        
        return status_data
    
    def is_terminal(status_data):
        return status_data['status'] in ['COMPLETED', 'FAILED']
    
    try:
        final_status = poll_until_sync(
            check_fn=check_status,
            condition_fn=is_terminal,
            max_attempts=60,
            initial_delay=2.0,
            timeout=300
        )
    except TimeoutError:
        pytest.fail("FAILED: Job stuck after orchestrator restart - did not reach terminal state")
    
    # ========================================================================
    # STEP 5: Validate terminal state reached
    # ========================================================================
    print("\n[5/5] Validating recovery...")
    
    final_state = final_status['status']
    assert_terminal_state(final_state)
    
    if final_state == 'FAILED':
        error_msg = final_status.get('error_message', 'Unknown error')
        print(f"   âš  Job failed after restart: {error_msg}")
        print(f"   âœ“ Failure is visible and terminal (acceptable)")
    else:
        print(f"   âœ“ Job completed successfully after restart")
    
    print("\n" + "="*80)
    print("âœ… RESILIENCE TEST PASSED - Platform survived orchestrator restart")
    print("="*80)
    print(f"Final State: {final_state}")
    print(f"Polls Required: {poll_count}")
    print("="*80 + "\n")


@pytest.mark.resilience
@pytest.mark.slow
def test_job_survives_pricing_engine_restart(api_client, track_correlation):
    """
    Test that jobs survive pricing-engine restart.
    
    HARD REQUIREMENTS:
    - Job MUST reach terminal state after restart
    - Job MUST NOT be stuck
    - Failure reason MUST be visible if job fails
    """
    print("\n" + "="*80)
    print("RESILIENCE TEST: Pricing Engine Restart Mid-Job")
    print("="*80)
    
    docker = DockerController()
    
    # ========================================================================
    # STEP 1: Create and start a job
    # ========================================================================
    print("\n[1/5] Creating job...")
    
    # Get usage profile
    profiles_response = api_client.get('/usage-profiles')
    assert profiles_response['success'], "FAILED: Could not fetch usage profiles"
    
    profile_id = profiles_response['data'][0]['id']
    
    # Upload Terraform
    fixture_dir = Path(__file__).parent.parent.parent / 'fixtures' / 'simple_ec2'
    assert fixture_dir.exists(), f"FAILED: Fixture not found: {fixture_dir}"
    
    from tests.test_07_e2e.test_full_user_flow import create_terraform_zip
    import requests
    
    files = {'file': ('terraform.zip', create_terraform_zip(fixture_dir), 'application/zip')}
    upload_response = requests.post(f"{api_client.base_url}/uploads", files=files)
    assert upload_response.status_code in [200, 201], "FAILED: Upload failed"
    
    upload_id = upload_response.json()['data']['upload_id']
    
    # Create job
    job_response = api_client.post('/jobs', json={
        'name': 'Resilience Test - Pricing Engine Restart',
        'upload_id': upload_id,
        'usage_profile': profile_id
    })
    assert job_response['success'], "FAILED: Job creation failed"
    
    job_id = job_response['data']['job_id']
    print(f"   âœ“ Job created: {job_id}")
    
    # ========================================================================
    # STEP 2: Wait for job to reach COSTING stage
    # ========================================================================
    print("\n[2/5] Waiting for job to reach COSTING stage...")
    
    max_wait = 30
    for i in range(max_wait):
        status_response = api_client.get(f'/jobs/{job_id}/status')
        current_status = status_response['data']['status']
        
        if current_status == 'COSTING':
            print(f"   âœ“ Job in COSTING stage")
            break
        
        if current_status in ['COMPLETED', 'FAILED']:
            print(f"   âš  Job reached terminal state before restart: {current_status}")
            pytest.skip("Job completed too quickly - cannot test pricing engine restart")
        
        time.sleep(1)
    else:
        print("   âš  Job did not reach COSTING stage - restarting anyway")
    
    # ========================================================================
    # STEP 3: Restart pricing-engine
    # ========================================================================
    print("\n[3/5] Restarting pricing-engine...")
    
    try:
        docker.restart_container('cost-platform-pricing-engine', wait_healthy=False, timeout=30)
    except Exception as e:
        pytest.fail(f"FAILED: Could not restart pricing-engine: {e}")
    
    print("   âœ“ Pricing engine restarted")
    
    # ========================================================================
    # STEP 4: Poll until terminal state
    # ========================================================================
    print("\n[4/5] Polling job to terminal state...")
    
    poll_count = 0
    
    def check_status():
        nonlocal poll_count
        poll_count += 1
        
        status_response = api_client.get(f'/jobs/{job_id}/status')
        assert status_response['success'], f"FAILED: Status check failed at poll {poll_count}"
        
        status_data = status_response['data']
        current_state = status_data['status']
        
        print(f"   Poll {poll_count}: {current_state} ({status_data.get('progress', 0)}%)")
        
        return status_data
    
    def is_terminal(status_data):
        return status_data['status'] in ['COMPLETED', 'FAILED']
    
    try:
        final_status = poll_until_sync(
            check_fn=check_status,
            condition_fn=is_terminal,
            max_attempts=60,
            initial_delay=2.0,
            timeout=300
        )
    except TimeoutError:
        pytest.fail("FAILED: Job stuck after pricing engine restart - did not reach terminal state")
    
    # ========================================================================
    # STEP 5: Validate terminal state reached
    # ========================================================================
    print("\n[5/5] Validating recovery...")
    
    final_state = final_status['status']
    assert_terminal_state(final_state)
    
    if final_state == 'FAILED':
        error_msg = final_status.get('error_message', 'Unknown error')
        print(f"   âš  Job failed after restart: {error_msg}")
        print(f"   âœ“ Failure is visible and terminal (acceptable)")
    else:
        print(f"   âœ“ Job completed successfully after restart")
    
    print("\n" + "="*80)
    print("âœ… RESILIENCE TEST PASSED - Platform survived pricing engine restart")
    print("="*80)
    print(f"Final State: {final_state}")
    print(f"Polls Required: {poll_count}")
    print("="*80 + "\n")





================================================================================
FILE: tests\03_uploads\test_upload_flow.py
TYPE: Test Suite
SIZE: 9920 bytes
MODIFIED: 12/24/2025 21:30:40
================================================================================

"""
Upload flow tests - REAL validation with HARD FAILURES.

These tests enforce that upload functionality is a REQUIRED platform capability:
- Upload API MUST exist
- Valid Terraform files MUST upload successfully
- Invalid files MUST be rejected
- upload_id MUST be valid UUID
- Uploads MUST be retrievable

ANY failure indicates platform is INCOMPLETE and blocks deployment.
"""
import pytest
import io
import zipfile
from pathlib import Path
from utils.assertions import assert_correlation_id


@pytest.mark.uploads
def test_upload_api_exists(api_client):
    """
    Test that upload API endpoint exists.
    
    CRITICAL: If this fails, platform is NOT functional.
    Upload capability is REQUIRED for platform certification.
    """
    import requests
    
    # Try to access upload endpoint
    response = requests.options(f"{api_client.base_url}/uploads")
    
    assert response.status_code != 404, \
        "FAILED: Upload API does not exist - platform is INCOMPLETE"
    
    print("   âœ“ Upload API endpoint exists")


@pytest.mark.uploads
def test_valid_terraform_upload(api_client, track_correlation):
    """
    Test uploading valid Terraform files.
    
    HARD REQUIREMENTS:
    - Upload MUST succeed
    - Response MUST contain upload_id
    - upload_id MUST be valid UUID
    - ApiResponse envelope MUST be valid
    """
    print("\n" + "="*60)
    print("UPLOAD TEST: Valid Terraform Upload")
    print("="*60)
    
    # Get fixture
    fixture_dir = Path(__file__).parent.parent.parent / 'fixtures' / 'simple_ec2'
    assert fixture_dir.exists(), f"FAILED: Fixture not found: {fixture_dir}"
    
    # Create ZIP
    zip_data = create_terraform_zip(fixture_dir)
    
    # Upload
    import requests
    files = {
        'file': ('terraform.zip', zip_data, 'application/zip')
    }
    
    upload_url = f"{api_client.base_url}/uploads"
    response = requests.post(upload_url, files=files)
    
    # Validate HTTP status
    assert response.status_code in [200, 201], \
        f"FAILED: Upload failed with status {response.status_code}. Response: {response.text}"
    
    # Validate response structure
    data = response.json()
    assert 'success' in data, "FAILED: Response missing 'success' field"
    assert 'correlation_id' in data, "FAILED: Response missing 'correlation_id'"
    
    assert_correlation_id(data)
    track_correlation(data, '/uploads', 'POST')
    
    # Validate success
    assert data['success'] is True, \
        f"FAILED: Upload returned success=false. Error: {data.get('error')}"
    
    # Validate upload_id
    assert 'data' in data, "FAILED: Response missing 'data' field"
    assert 'upload_id' in data['data'], "FAILED: Response missing 'upload_id'"
    
    upload_id = data['data']['upload_id']
    
    # Validate UUID format
    import uuid
    try:
        uuid.UUID(upload_id)
    except ValueError:
        pytest.fail(f"FAILED: upload_id is not a valid UUID: {upload_id}")
    
    print(f"   âœ“ Upload successful: {upload_id}")
    print("="*60 + "\n")
    
    return upload_id


@pytest.mark.uploads
def test_upload_persistence(api_client):
    """
    Test that uploads are retrievable after creation.
    
    HARD REQUIREMENTS:
    - Upload MUST be retrievable via GET /uploads/{id}
    - Retrieved data MUST match uploaded data
    """
    print("\n" + "="*60)
    print("UPLOAD TEST: Upload Persistence")
    print("="*60)
    
    # Create upload
    fixture_dir = Path(__file__).parent.parent.parent / 'fixtures' / 'simple_ec2'
    zip_data = create_terraform_zip(fixture_dir)
    
    import requests
    files = {'file': ('terraform.zip', zip_data, 'application/zip')}
    upload_response = requests.post(f"{api_client.base_url}/uploads", files=files)
    
    assert upload_response.status_code in [200, 201], "FAILED: Upload failed"
    
    upload_id = upload_response.json()['data']['upload_id']
    print(f"   Upload created: {upload_id}")
    
    # Retrieve upload
    get_response = api_client.get(f'/uploads/{upload_id}')
    
    assert get_response['success'], \
        f"FAILED: Could not retrieve upload {upload_id}. Error: {get_response.get('error')}"
    
    upload_data = get_response['data']
    
    # Validate upload data
    assert 'upload_id' in upload_data, "FAILED: Retrieved upload missing 'upload_id'"
    assert upload_data['upload_id'] == upload_id, \
        f"FAILED: upload_id mismatch. Expected: {upload_id}, Got: {upload_data['upload_id']}"
    
    print(f"   âœ“ Upload retrieved successfully")
    print("="*60 + "\n")


@pytest.mark.uploads
def test_invalid_file_upload_rejected(api_client):
    """
    Test that invalid files are rejected.
    
    HARD REQUIREMENTS:
    - Non-ZIP files MUST be rejected
    - Response MUST indicate failure
    - Error message MUST be present
    """
    print("\n" + "="*60)
    print("UPLOAD TEST: Invalid File Rejection")
    print("="*60)
    
    # Try to upload a text file instead of ZIP
    import requests
    files = {
        'file': ('invalid.txt', b'This is not a valid Terraform file', 'text/plain')
    }
    
    response = requests.post(f"{api_client.base_url}/uploads", files=files)
    
    # Should either reject with 400/422 or return success=false
    if response.status_code in [400, 422]:
        print(f"   âœ“ Invalid file rejected with status {response.status_code}")
    else:
        # Check response
        data = response.json()
        
        # If it returns 200, it MUST have success=false
        if response.status_code == 200:
            assert data.get('success') is False, \
                "FAILED: Invalid file upload returned success=true"
            
            assert 'error' in data, "FAILED: Error response missing 'error' field"
            assert data['error'] is not None, "FAILED: Error field is null"
            
            print(f"   âœ“ Invalid file rejected: {data['error'].get('message')}")
        else:
            pytest.fail(f"FAILED: Unexpected status code {response.status_code}")
    
    print("="*60 + "\n")


@pytest.mark.uploads
def test_empty_file_upload_rejected(api_client):
    """
    Test that empty files are rejected.
    
    HARD REQUIREMENTS:
    - Empty files MUST be rejected
    - Error MUST be clear
    """
    print("\n" + "="*60)
    print("UPLOAD TEST: Empty File Rejection")
    print("="*60)
    
    import requests
    files = {
        'file': ('empty.zip', b'', 'application/zip')
    }
    
    response = requests.post(f"{api_client.base_url}/uploads", files=files)
    
    # Should reject empty files
    if response.status_code in [400, 422]:
        print(f"   âœ“ Empty file rejected with status {response.status_code}")
    else:
        data = response.json()
        
        if response.status_code == 200:
            assert data.get('success') is False, \
                "FAILED: Empty file upload returned success=true"
            
            print(f"   âœ“ Empty file rejected: {data['error'].get('message')}")
        else:
            pytest.fail(f"FAILED: Unexpected status code {response.status_code}")
    
    print("="*60 + "\n")


@pytest.mark.uploads
def test_malformed_zip_rejected(api_client):
    """
    Test that malformed ZIP files are rejected.
    
    HARD REQUIREMENTS:
    - Corrupted ZIP files MUST be rejected
    - Error MUST be clear
    """
    print("\n" + "="*60)
    print("UPLOAD TEST: Malformed ZIP Rejection")
    print("="*60)
    
    import requests
    
    # Create malformed ZIP (just random bytes)
    malformed_zip = b'PK\x03\x04' + b'\x00' * 100  # Looks like ZIP header but is invalid
    
    files = {
        'file': ('malformed.zip', malformed_zip, 'application/zip')
    }
    
    response = requests.post(f"{api_client.base_url}/uploads", files=files)
    
    # Should reject malformed files
    if response.status_code in [400, 422]:
        print(f"   âœ“ Malformed ZIP rejected with status {response.status_code}")
    else:
        data = response.json()
        
        if response.status_code == 200:
            assert data.get('success') is False, \
                "FAILED: Malformed ZIP upload returned success=true"
            
            print(f"   âœ“ Malformed ZIP rejected: {data['error'].get('message')}")
        else:
            pytest.fail(f"FAILED: Unexpected status code {response.status_code}")
    
    print("="*60 + "\n")


@pytest.mark.uploads
def test_upload_without_file_rejected(api_client):
    """
    Test that requests without file are rejected.
    
    HARD REQUIREMENTS:
    - Missing file MUST be rejected
    - Error MUST be clear
    """
    print("\n" + "="*60)
    print("UPLOAD TEST: Missing File Rejection")
    print("="*60)
    
    import requests
    
    # POST without file
    response = requests.post(f"{api_client.base_url}/uploads")
    
    # Should reject
    assert response.status_code in [400, 422], \
        f"FAILED: Upload without file should return 400/422, got {response.status_code}"
    
    print(f"   âœ“ Request without file rejected with status {response.status_code}")
    print("="*60 + "\n")


def create_terraform_zip(fixture_dir: Path) -> bytes:
    """
    Create a ZIP file from Terraform fixtures.
    
    Args:
        fixture_dir: Path to fixture directory
        
    Returns:
        ZIP file bytes
    """
    zip_buffer = io.BytesIO()
    
    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
        for tf_file in fixture_dir.glob('*.tf'):
            zip_file.write(tf_file, arcname=tf_file.name)
    
    zip_buffer.seek(0)
    return zip_buffer.read()





================================================================================
FILE: tests\02_api_gateway\test_usage_profiles.py
TYPE: Test Suite
SIZE: 1500 bytes
MODIFIED: 12/24/2025 21:11:55
================================================================================

"""
API Gateway tests - authentication, rate limiting, validation.
"""
import pytest


@pytest.mark.api
def test_usage_profiles_endpoint(api_client, track_correlation):
    """Test that usage profiles endpoint works."""
    response = api_client.get('/usage-profiles')
    track_correlation(response, '/usage-profiles', 'GET')
    
    assert response['success'], "Failed to fetch usage profiles"
    assert isinstance(response['data'], list), "Usage profiles should be a list"


@pytest.mark.api
def test_cors_headers_present(api_client):
    """Test that CORS headers are present (for browser clients)."""
    import requests
    
    # Make OPTIONS request (preflight)
    response = requests.options(
        f"{api_client.base_url}/usage-profiles",
        headers={'Origin': 'http://localhost:3000'}
    )
    
    # Check for CORS headers
    assert 'Access-Control-Allow-Origin' in response.headers, \
        "Missing CORS header: Access-Control-Allow-Origin"


@pytest.mark.api
def test_input_validation_returns_error(api_client):
    """Test that invalid input returns proper error response."""
    # Try to create job with invalid data
    import requests
    
    response = requests.post(
        f"{api_client.base_url}/jobs",
        json={'invalid': 'data'}
    )
    
    # Should return error response (not 500)
    assert response.status_code in [400, 422], \
        f"Expected 400/422 for invalid input, got {response.status_code}"





================================================================================
FILE: README.md
TYPE: Documentation
SIZE: 8201 bytes
MODIFIED: 12/24/2025 21:12:58
================================================================================

# Platform Tester Service

A comprehensive black-box testing service that validates the entire Terraform Cost Intelligence Platform end-to-end.

## Overview

The Platform Tester treats the entire system as a distributed black box and validates:
- **Contract compliance**: All API responses match JSON schemas
- **State machine correctness**: Job transitions are valid and monotonic
- **Immutability**: Results are write-once and never change
- **Resilience**: Platform survives service restarts
- **End-to-end flow**: Complete user journey works correctly

## Architecture

```
platform-tester (container)
â”œâ”€â”€ Health Tests â†’ Ensure all services reachable
â”œâ”€â”€ Contract Tests â†’ Validate API response schemas
â”œâ”€â”€ API Gateway Tests â†’ Auth, rate limiting, validation
â”œâ”€â”€ Job Tests â†’ Lifecycle, state transitions
â”œâ”€â”€ Terraform Tests â†’ Sandbox constraints
â”œâ”€â”€ Cost Pipeline Tests â†’ Metadata, pricing, usage, aggregation
â”œâ”€â”€ Results Tests â†’ Persistence, immutability
â”œâ”€â”€ E2E Tests â†’ Full user flow
â””â”€â”€ Resilience Tests â†’ Service restart recovery
```

## Quick Start

### Run All Tests

```bash
# Local
docker compose up --build platform-tester

# CI (blocks deployment on failure)
docker compose up --abort-on-container-exit --exit-code-from platform-tester
```

### Run Specific Test Category

```bash
# Health checks only
docker compose run platform-tester pytest tests/00_health/ -v

# Contract tests only
docker compose run platform-tester pytest tests/01_contracts/ -v

# E2E tests only
docker compose run platform-tester pytest tests/07_e2e/ -v
```

### Run with Markers

```bash
# Run all health tests
docker compose run platform-tester pytest -m health -v

# Run all contract tests
docker compose run platform-tester pytest -m contract -v

# Run all E2E tests
docker compose run platform-tester pytest -m e2e -v

# Skip slow tests
docker compose run platform-tester pytest -m "not slow" -v
```

## Test Categories

### 00_health - Health Checks
- Nginx responds
- API Gateway healthy
- All backend services reachable

**Purpose**: Fail fast if infrastructure is down

### 01_contracts - Contract Validation
- ApiResponse envelope structure
- correlation_id presence and format
- success/error semantics
- No forbidden fields

**Purpose**: Ensure API contract compliance

### 02_api_gateway - API Gateway
- Authentication (JWT validation)
- Rate limiting enforcement
- Input validation
- CORS headers

**Purpose**: Validate gateway functionality

### 03_jobs - Job Lifecycle
- Job creation
- State transitions
- Progress monotonicity
- Terminal states

**Purpose**: Validate job state machine

### 04_terraform_execution - Terraform Sandbox
- Prevent local-exec
- Prevent file writes
- Prevent network access

**Purpose**: Ensure sandbox security

### 05_cost_pipeline - Cost Pipeline
- Metadata enrichment
- Pricing resolution
- Usage modeling
- Cost aggregation

**Purpose**: Validate cost calculation correctness

### 06_results_governance - Results Governance
- Write-once enforcement
- Update/delete rejection
- Persistence across restarts

**Purpose**: Ensure immutability

### 07_e2e - End-to-End
- Full user flow
- Upload â†’ Create â†’ Poll â†’ Results
- Browser refresh mid-job
- Result immutability

**Purpose**: Validate complete user journey

### 08_resilience - Resilience
- Service restart recovery
- Partial failure handling
- Job resume/fail cleanly

**Purpose**: Validate platform resilience

## Exit Codes

- **0**: All tests passed â†’ Deploy
- **1**: Tests failed â†’ Block deployment

## Configuration

### Environment Variables

```bash
BASE_URL=http://nginx          # Nginx base URL
API_BASE=http://nginx/api      # API Gateway base URL
PYTEST_ARGS="-v --tb=short"    # Pytest arguments
```

### Endpoints (config/endpoints.yaml)

```yaml
endpoints:
  health: /health
  api_health: /api/health
  usage_profiles: /api/usage-profiles
  jobs: /api/jobs
  job_status: /api/jobs/{job_id}/status
  job_results: /api/jobs/{job_id}/results
```

## JSON Schemas

All API responses are validated against JSON schemas in `config/contracts/`:

- **ApiResponse.json**: Standard response envelope
- **Job.json**: Job object structure
- **UsageProfile.json**: Usage profile structure
- **CostResult.json**: Cost result structure

## Utilities

### API Client (`utils/api_client.py`)

```python
from utils.api_client import PlatformClient

client = PlatformClient()
response = client.get('/usage-profiles', validate_schema='UsageProfile')
```

Features:
- Automatic schema validation
- correlation_id tracking
- Error handling

### Polling (`utils/polling.py`)

```python
from utils.polling import poll_until_sync

result = poll_until_sync(
    check_fn=lambda: client.get(f'/jobs/{job_id}/status'),
    condition_fn=lambda r: r['data']['status'] in ['COMPLETED', 'FAILED'],
    max_attempts=60,
    timeout=300
)
```

Features:
- Exponential backoff
- Timeout handling
- Condition checking

### Assertions (`utils/assertions.py`)

```python
from utils.assertions import (
    assert_valid_state_transition,
    assert_correlation_id,
    assert_terminal_state
)

assert_valid_state_transition('PLANNING', 'PARSING')  # OK
assert_valid_state_transition('COMPLETED', 'PARSING')  # AssertionError
```

### Correlation Tracking (`utils/correlation.py`)

```python
from utils.correlation import track_request, print_correlation_summary

track_request(correlation_id, '/jobs', 'POST', success=True)
print_correlation_summary()  # At end of tests
```

## Development

### Adding New Tests

1. Create test file in appropriate category directory
2. Use `@pytest.mark.<category>` decorator
3. Use `api_client` fixture for API calls
4. Use `track_correlation` fixture to track correlation IDs
5. Validate responses against schemas

Example:

```python
import pytest

@pytest.mark.jobs
def test_job_creation(api_client, track_correlation):
    """Test job creation."""
    response = api_client.post('/jobs', json={
        'name': 'Test Job',
        'upload_id': 'uuid',
        'usage_profile': 'prod'
    }, validate_schema='Job')
    
    track_correlation(response, '/jobs', 'POST')
    
    assert response['success']
    assert 'job_id' in response['data']
```

### Running Tests Locally

```bash
# Install dependencies
cd tester
pip install -r requirements.txt

# Run tests (requires platform running)
export BASE_URL=http://localhost
export API_BASE=http://localhost/api
pytest -v
```

## CI/CD Integration

### GitHub Actions Example

```yaml
- name: Run Platform Tests
  run: |
    docker compose up -d
    docker compose up --abort-on-container-exit --exit-code-from platform-tester
    
- name: Cleanup
  if: always()
  run: docker compose down
```

### GitLab CI Example

```yaml
test:
  script:
    - docker compose up -d
    - docker compose up --abort-on-container-exit --exit-code-from platform-tester
  after_script:
    - docker compose down
```

## Troubleshooting

### Tests Fail with "Nginx not healthy"

**Cause**: Nginx not started or not accessible

**Solution**:
```bash
# Check nginx status
docker compose ps nginx

# Check nginx logs
docker compose logs nginx

# Restart nginx
docker compose restart nginx
```

### Tests Fail with "Schema validation failed"

**Cause**: API response doesn't match schema

**Solution**:
1. Check correlation_id in error message
2. Review API response structure
3. Update schema if API changed intentionally

### Tests Timeout

**Cause**: Job taking too long or stuck

**Solution**:
1. Check job-orchestrator logs
2. Increase timeout in test
3. Check for deadlocks in backend services

## Success Criteria

When platform-tester passes:
âœ… Platform is production-ready
âœ… All contracts enforced
âœ… State machine correct
âœ… Results immutable
âœ… Platform resilient
âœ… E2E flow works

## License

Same as parent project





================================================================================
FILE: docker-compose-snippet.yml
TYPE: Other
SIZE: 1518 bytes
MODIFIED: 12/24/2025 22:08:31
================================================================================

version: '3.9'

# ================================================================================
# TERRAFORM COST INTELLIGENCE PLATFORM - DOCKER COMPOSE
# ================================================================================

networks:
  cost-platform:
    driver: bridge

volumes:
  postgres_data:
  redis_data:


services:
  # ... (all existing services remain unchanged)

  # ============================================================================
  # PLATFORM TESTER - Quality Gate & Certification
  # ============================================================================
  # Black-box testing service that validates the entire platform
  # Runs once and exits with code 0 (pass) or 1 (fail)
  # Set CERTIFICATION_MODE=true to forbid test skips
  # ============================================================================

  platform-tester:
    build:
      context: ./tester
      dockerfile: Dockerfile
    container_name: cost-platform-tester
    networks:
      - cost-platform
    depends_on:
      nginx:
        condition: service_started
      api-gateway:
        condition: service_healthy
      job-orchestrator:
        condition: service_healthy
    environment:
      BASE_URL: http://nginx
      API_BASE: http://nginx/api
      PYTEST_ARGS: "-v --tb=short --color=yes"
      CERTIFICATION_MODE: ${CERTIFICATION_MODE:-false} # Set to 'true' for strict mode
    restart: "no" # Quality gate, not a daemon
    profiles:







================================================================================
FILE: fixtures\simple_ec2\main.tf
TYPE: Other
SIZE: 680 bytes
MODIFIED: 12/24/2025 21:23:19
================================================================================

# Simple EC2 instance for E2E testing
# This configuration produces a deterministic plan without requiring real AWS credentials

resource "aws_instance" "test_instance" {
  ami           = "ami-0c55b159cbfafe1f0" # Amazon Linux 2 AMI (us-east-1)
  instance_type = "t3.micro"

  tags = {
    Name        = "E2E-Test-Instance"
    Environment = "test"
    ManagedBy   = "platform-tester"
  }
}

# Output for verification
output "instance_id" {
  description = "The ID of the EC2 instance"
  value       = aws_instance.test_instance.id
}

output "instance_type" {
  description = "The instance type"
  value       = aws_instance.test_instance.instance_type
}





================================================================================
FILE: fixtures\simple_ec2\providers.tf
TYPE: Other
SIZE: 477 bytes
MODIFIED: 12/24/2025 21:22:41
================================================================================

terraform {
  required_version = ">= 1.0"
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

provider "aws" {
  region = "us-east-1"
  
  # Skip credentials for plan-only execution
  skip_credentials_validation = true
  skip_requesting_account_id  = true
  skip_metadata_api_check     = true
  
  # Use fake credentials for planning
  access_key = "mock_access_key"
  secret_key = "mock_secret_key"
}





================================================================================
SUMMARY
================================================================================
Total Files Processed: 26
Total Size: 78.42 KB
Output File: tester-combined.txt
Completed: 2025-12-24 22:21:01

FILE BREAKDOWN:
- Configuration Files: 7
- JSON Schemas: 4
- Utility Modules: 6
- Test Files: 5
- Documentation: 1
================================================================================
